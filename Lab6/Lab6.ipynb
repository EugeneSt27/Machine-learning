{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad2b5d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "857c3978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ea35498-67b7-4696-a6b8-90151320152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4bcc6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e142c188-5212-463a-abce-a840ba570848",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80311930",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('comments.csv')\n",
    "df.drop('Unnamed: 0', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "484097ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "toxic    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "99272597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1429e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    143106\n",
       "1     16186\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['toxic'].value_counts() # Дисбаланс классов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5c474e",
   "metadata": {},
   "source": [
    "# Предобработка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b6ef7a",
   "metadata": {},
   "source": [
    "### Методы предобработки текста\n",
    "1. Удаление HTML-тегов\n",
    "2. Удаление знаков препинания\n",
    "3. Приведение к нижнему регистру\n",
    "4. Удаление стоп-слов: Стоп-слова — это часто встречающиеся слова (например, \"и\", \"в\", \"на\")\n",
    "5. Лемматизация или стемминг: Эти методы приводят слова к их базовой форме (лемме или корню)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0c27aaf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66460f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.sub('<.*?>', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text).lower()\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]  #if word not in stop_words] пока не будем удалять стоп-слова\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33c96a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a7f16119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>explanation why the edits made under my userna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>daww he match this background colour im seemin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>hey man im really not trying to edit war it ju...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>more i cant make any real suggestion on improv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>you sir are my hero any chance you remember wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159287</th>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>and for the second time of asking when your vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159288</th>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>you should be ashamed of yourself that is a ho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159289</th>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>spitzer umm there no actual article for prosti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159290</th>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>and it look like it wa actually you who put on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159291</th>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>and i really dont think you understand i came ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159292 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  toxic  \\\n",
       "0       Explanation\\nWhy the edits made under my usern...      0   \n",
       "1       D'aww! He matches this background colour I'm s...      0   \n",
       "2       Hey man, I'm really not trying to edit war. It...      0   \n",
       "3       \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4       You, sir, are my hero. Any chance you remember...      0   \n",
       "...                                                   ...    ...   \n",
       "159287  \":::::And for the second time of asking, when ...      0   \n",
       "159288  You should be ashamed of yourself \\n\\nThat is ...      0   \n",
       "159289  Spitzer \\n\\nUmm, theres no actual article for ...      0   \n",
       "159290  And it looks like it was actually you who put ...      0   \n",
       "159291  \"\\nAnd ... I really don't think you understand...      0   \n",
       "\n",
       "                                        preprocessed_text  \n",
       "0       explanation why the edits made under my userna...  \n",
       "1       daww he match this background colour im seemin...  \n",
       "2       hey man im really not trying to edit war it ju...  \n",
       "3       more i cant make any real suggestion on improv...  \n",
       "4       you sir are my hero any chance you remember wh...  \n",
       "...                                                   ...  \n",
       "159287  and for the second time of asking when your vi...  \n",
       "159288  you should be ashamed of yourself that is a ho...  \n",
       "159289  spitzer umm there no actual article for prosti...  \n",
       "159290  and it look like it wa actually you who put on...  \n",
       "159291  and i really dont think you understand i came ...  \n",
       "\n",
       "[159292 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1457541",
   "metadata": {},
   "source": [
    "# Векторизация Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b88209-3f3a-4f76-8b43-aa4161f82585",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = None\n",
    "try:\n",
    "    glove_model = api.load(\"glove-wiki-gigaword-100\")\n",
    "except api.exceptions.DownloadError as e:\n",
    "    print(f\"Error loading GloVe model: {e}\")\n",
    "    print(\"Please ensure you have an internet connection and try again.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a493b522",
   "metadata": {},
   "source": [
    "Датасет весьма большой, поэтому начнем с эмбеддингов слов, если будет плохо, то попробуем TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446f3228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment_vector(words):\n",
    "    vectors = [glove_model[word] for word in words if word in glove_model]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(glove_model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27eb52b-d4c2-4b00-b367-eb74f6802485",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['vector'] = df['preprocessed_text'].apply(get_comment_vector)\n",
    "X = np.array(df['vector'].tolist())\n",
    "y = df['toxic']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"\\nTraining data vectors shape:\", X_train.shape)\n",
    "print(\"Testing data vectors shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0027e02-66b8-4226-bd59-11836d689f73",
   "metadata": {},
   "source": [
    "# Построение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94361d66-bf4e-416c-b430-5362c1345cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, \n",
    "                                random_state=42,  \n",
    "                               class_weight='balanced'\n",
    "                              )\n",
    "model.fit(X_train, y_train)\n",
    "y_approx = model.predict(X_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5038f979-d9a1-4cc0-a8b8-59ffa21efb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ecb3bf-26aa-46c4-923a-5599818291ff",
   "metadata": {},
   "source": [
    "Пробовал разные варианты моделей классического ML, f1_score слишком маленький, поэтому идем к BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6839ba48-d4d9-43cb-83db-affad534b250",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "03d4aaa6-b5d7-4554-8688-41d5bd515457",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicComments():\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts.iloc[item])\n",
    "        label = self.labels.iloc[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        return_token_type_ids=False,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5cce73b9-8f4f-4fbb-b27b-8a7304319b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert_with_f1(model, dataloader, device, num_epochs=3):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        all_predicted_labels = []\n",
    "        all_true_labels = []\n",
    "        progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}', unit='batch')\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            _, predicted_labels = torch.max(logits, dim=1)\n",
    "            # Собираем предсказанные и истинные метки для вычисления F1-score\n",
    "            all_predicted_labels.extend(predicted_labels.cpu().tolist())\n",
    "            all_true_labels.extend(labels.cpu().tolist())\n",
    "            progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        train_f1 = f1_score(all_true_labels, all_predicted_labels, average='binary')  # Для бинарной классификации\n",
    "        print(f'\\nEpoch {epoch+1} завершена. Средний loss: {avg_loss:.4f}, F1-score на обучающей выборке: {train_f1:.4f}')\n",
    "\n",
    "    print('\\nОбучение завершено!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d663b24b-5af4-40cb-b467-76cf35e09025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2fdf936c-d612-497c-8cdf-796de16ff8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_BERT, X_test_BERT, y_train_BERT, y_test_BERT = train_test_split(df['text'], df['toxic'], test_size=0.2)\n",
    "MAX_LEN = 128\n",
    "datasets = [X_train_BERT, X_test_BERT, y_train_BERT, y_test_BERT]\n",
    "for ds in datasets:\n",
    "    ds.reset_index(drop = True, inplace = True)\n",
    "train_dataset = ToxicComments(X_train_BERT, y_train_BERT, tokenizer, MAX_LEN)\n",
    "test_dataset = ToxicComments(X_test_BERT, y_test_BERT, tokenizer, MAX_LEN)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aa1f080c-f126-4820-a70e-c971c225e45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающего датасета: 127433\n",
      "Количество батчей в DataLoader: 7965\n"
     ]
    }
   ],
   "source": [
    "print(f\"Размер обучающего датасета: {len(train_dataset)}\")\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "print(f\"Количество батчей в DataLoader: {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "0efa9ce9-921c-4e3d-8a7a-f6369632183a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|█████████████████████████████████████████████████| 7965/7965 [9:41:10<00:00,  4.38s/batch, loss=0.1015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 завершена. Средний loss: 0.0938, F1-score на обучающей выборке: 0.8140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|█████████████████████████████████████████████████| 7965/7965 [9:33:01<00:00,  4.32s/batch, loss=0.0244]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 завершена. Средний loss: 0.0578, F1-score на обучающей выборке: 0.8824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|█████████████████████████████████████████████████| 7965/7965 [9:38:20<00:00,  4.36s/batch, loss=0.0076]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 завершена. Средний loss: 0.0318, F1-score на обучающей выборке: 0.9411\n",
      "\n",
      "Обучение завершено!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "train_bert_with_f1(model, train_dataloader, device, num_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a85545-bed3-4de9-a520-23824ad92f74",
   "metadata": {},
   "source": [
    "мой прекрасный торч почему-то не видел gpu и обучал на cpu больше суток..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fe9af27b-6000-41b0-8539-3e2da833797b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель сохранена в: ./toxic_comment_model\n",
      "Токенизатор сохранен в: ./toxic_comment_tokenizer\n"
     ]
    }
   ],
   "source": [
    "output_dir = './toxic_comment_model'  # Укажите путь, куда вы хотите сохранить модель\n",
    "output_tokenizer_dir = './toxic_comment_tokenizer' # Укажите путь для токенизатора\n",
    "\n",
    "# Сохранение модели\n",
    "model.save_pretrained(output_dir)\n",
    "# Сохранение токенизатора\n",
    "tokenizer.save_pretrained(output_tokenizer_dir)\n",
    "\n",
    "print(f\"Модель сохранена в: {output_dir}\")\n",
    "print(f\"Токенизатор сохранен в: {output_tokenizer_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6795a6d9-e7c1-4be1-8619-fc61d0a7209f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a87904ce-bd3b-4e10-85fc-fc1d93879577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f9a9ae53-0b2a-47a2-be0e-da94ec840d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель успешно загружена из: ./toxic_comment_model\n",
      "Токенизатор успешно загружен из: ./toxic_comment_tokenizer\n"
     ]
    }
   ],
   "source": [
    "model_path = './toxic_comment_model'\n",
    "tokenizer_path = './toxic_comment_tokenizer'\n",
    "\n",
    "try:\n",
    "    # Загрузка предварительно обученной модели для классификации последовательностей\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    print(f\"Модель успешно загружена из: {model_path}\")\n",
    "\n",
    "    # Загрузка предварительно обученного токенизатора\n",
    "    tokenizer = BertTokenizer.from_pretrained(tokenizer_path)\n",
    "    print(f\"Токенизатор успешно загружен из: {tokenizer_path}\")\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Произошла ошибка при загрузке модели или токенизатора: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9fcfeecc-6093-4e7d-949a-c5c91b659c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = []\n",
    "actual_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        _, preds = torch.max(outputs.logits, dim=1)\n",
    "        predictions.extend(preds.cpu().tolist())\n",
    "        actual_labels.extend(labels.cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a9b244c8-b438-4f4e-bb9c-446f147cfb9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99     28658\n",
      "           1       0.96      0.93      0.94      3201\n",
      "\n",
      "    accuracy                           0.99     31859\n",
      "   macro avg       0.98      0.96      0.97     31859\n",
      "weighted avg       0.99      0.99      0.99     31859\n",
      "\n",
      "f1: 0.9444532866465064\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(actual_labels, predictions))\n",
    "print(f\"f1: {f1_score(actual_labels, predictions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911076a3-3406-4dc6-811c-9bbdcd0e69bf",
   "metadata": {},
   "source": [
    "Результаты превзошли все ожидания, слава трансформерам!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
